model:
  # BitNet Text Encoder (fully quantized)
  vocab_size: 50257  # GPT-2 vocabulary
  text_encoder_dim: 512
  text_encoder_layers: 6
  text_encoder_heads: 8
  
  # BitNet Text Decoder (fully quantized)
  text_decoder_dim: 512
  text_decoder_layers: 6
  text_decoder_heads: 8
  
  # Vision processing (DiNOv2 features)
  vision_encoder_dim: 768  # DiNOv2 feature dimension
  vision_latent_size: 512
  vision_hidden_size: 256
  
  # Cross-modal fusion (BitNet quantized)
  fusion_hidden_size: 512
  fusion_num_heads: 8
  fusion_num_layers: 2
  
  # Episodic Memory (Larimar-inspired)
  memory_size: 512  # Number of memory slots
  episode_dim: 512  # Episode feature dimension
  memory_alpha: 0.1
  direct_writing: true
  
  # Model configuration
  max_seq_len: 128  # Shorter for efficiency
  dropout: 0.1
  fusion_num_layers: 2
  
  # Quantization settings
  weight_quantization: "ternary"  # {-1, 0, +1}
  activation_quantization: "int8"
  gradient_checkpointing: true
  
  # Training hyperparameters
  learning_rate: 1e-4
  weight_decay: 0.01
  warmup_steps: 1000
  gradient_clip: 1.0
  beta: 0.5
  use_beta_schedule: true
  ratio_increase: 0.25
  ratio_zero: 0.5
  
  # Attention analysis
  track_attention: true
  save_attention_patterns: true
  analyze_cross_modal: true

data:
  # Dataset paths
  captions_file: "../babylm_dataset/cc_3M_captions.json"
  vision_features_1: "../babylm_dataset/cc_3M_dino_v2_states_1of2.npy"
  vision_features_2: "../babylm_dataset/cc_3M_dino_v2_states_2of2.npy"
  
  # Data processing
  max_seq_length: 512
  vision_feature_dim: 768
  train_split: 0.95
  val_split: 0.05
  
  # DataLoader settings
  batch_size: 16
  num_workers: 4
  pin_memory: true
  persistent_workers: true

training:
  # Training configuration
  max_epochs: 10
  accumulate_grad_batches: 1
  precision: "16-mixed"
  
  # Validation and checkpointing
  val_check_interval: 0.25
  check_val_every_n_epoch: 1
  enable_checkpointing: true
  
  # Hardware settings
  accelerator: "auto"
  devices: "auto"
  strategy: "auto"
  
  # Optimization
  optimizer: "adamw"
  scheduler: "cosine"
  min_lr: 1e-6

# Wandb configuration
wandb:
  project: "bitmar-babylm"
  entity: "babylm-ntust-org"
  api_key: "5fba3726e4e32540d9fcba403f880dfaad983051"
  log_every_n_steps: 50
  log_attention: true
  log_memory: true
  log_gradients: false
  save_code: true

# Evaluation settings
evaluation:
  metrics: ["bleu", "rouge", "cross_modal_accuracy"]
  generate_samples: true
  num_samples: 100
  max_generation_length: 100
  temperature: 0.7
  top_p: 0.9

# Output directories
output:
  checkpoint_dir: "checkpoints"
  log_dir: "logs"
  attention_dir: "attention_analysis"
  memory_dir: "memory_analysis"
  results_dir: "results"

# CPU testing configuration
cpu_test:
  batch_size: 2
  max_samples: 100
  reduced_model_size: true
  memory_size: 64  # Reduced for CPU testing
