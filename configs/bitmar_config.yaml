model:
  # BitNet Text Encoder (fully quantized)
  vocab_size: 50257  # GPT-2 vocabulary
  text_encoder_dim: 512
  text_encoder_layers: 6
  text_encoder_heads: 8
  
  # BitNet Text Decoder (fully quantized)
  text_decoder_dim: 512
  text_decoder_layers: 6
  text_decoder_heads: 8
  
  # Vision processing (DiNOv2 features)
  vision_encoder_dim: 768  # DiNOv2 feature dimension
  vision_latent_size: 512
  vision_hidden_size: 256
  
  # Cross-modal fusion (BitNet quantized)
  fusion_hidden_size: 512
  fusion_num_heads: 8
  fusion_num_layers: 2
  
  # Episodic Memory (Larimar-inspired)
  memory_size: 512  # Number of memory slots
  episode_dim: 512  # Episode feature dimension (should match vision_latent_size)
  memory_alpha: 0.1
  direct_writing: true
  
  # Model configuration
  max_seq_len: 128  # Shorter for efficiency
  dropout: 0.1

data:
  # Dataset configuration
  dataset_dir: "../babylm_dataset"
  text_encoder_name: "gpt2"
  max_seq_length: 128  # Shorter for efficiency
  
  # DataLoader settings (optimized for RTX 4090)
  batch_size: 32  # Increased for RTX 4090
  num_workers: 8  # More workers for faster data loading
  pin_memory: true
  persistent_workers: true
  
  # HuggingFace configuration (use environment variables)
  hf_token: null  # Use HF_TOKEN environment variable
  validation_datasets: 
    - "squad"  # Public dataset, no auth required
    - "glue/sst2"  # Public dataset, no auth required

# Enhanced attention analysis configuration
attention_analysis:
  # Enable attention head tracking and visualization
  track_top_k: 20  # Track top 20 most important heads
  log_every_n_steps: 100  # Analyze attention every 100 steps
  viz_every_n_epochs: 2  # Create visualizations every 2 epochs
  save_head_patterns: true  # Save attention patterns to files
  analyze_memory_attention: true  # Analyze episodic memory attention
  analyze_cross_modal: true  # Analyze cross-modal attention fusion

training:
  # Training configuration
  max_epochs: 10
  accumulate_grad_batches: 1
  gradient_clip_val: 1.0
  val_check_interval: 1000
  scheduler: "cosine"
  min_lr: 0.00001
  warmup_steps: 1000
  learning_rate: 0.0001
  weight_decay: 0.01
  track_attention: true

# Weights & Biases configuration (optional)
# Enhanced wandb configuration with comprehensive logging
wandb:
  project: "bitmar-babylm"
  entity: "babylm-ntust"  # Use babylm-ntust team
  api_key: null  # Use WANDB_API_KEY environment variable
  log_every_n_steps: 50
  # Detailed logging categories with proper axis labels
  log_attention: true
  log_memory: true
  log_gradients: true
  log_quantization: true
  log_features: true
  save_code: true
  # Visualization settings
  create_plots: true
  plot_attention_heatmaps: true
  plot_memory_usage: true
  plot_quantization_dist: true

# Evaluation settings
evaluation:
  metrics: ["bleu", "rouge", "cross_modal_accuracy"]
  generate_samples: true
  num_samples: 100
  max_generation_length: 100
  temperature: 0.7
  top_p: 0.9

# Output directories (including attention analysis)
output:
  checkpoint_dir: "checkpoints"
  log_dir: "logs"
  attention_dir: "attention_analysis"
  memory_dir: "memory_analysis"
  results_dir: "results"
  attention_dir: "attention_analysis"
  memory_dir: "memory_analysis"
  results_dir: "results"

# CPU testing configuration
cpu_test:
  batch_size: 2
  max_samples: 100
  reduced_model_size: true
  memory_size: 64  # Reduced for CPU testing