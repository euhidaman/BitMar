model:
  # BitNet Text Encoder (Tiny Model - Quantized)
  vocab_size: 50257  # GPT-2 vocabulary
  text_encoder_dim: 256  # Reduced from 512 for edge efficiency
  text_encoder_layers: 4  # Reduced from 6 layers
  text_encoder_heads: 4  # Reduced from 8 heads
  
  # BitNet Text Decoder (Tiny Model - Quantized)
  text_decoder_dim: 256  # Reduced from 512 for edge efficiency
  text_decoder_layers: 4  # Reduced from 6 layers
  text_decoder_heads: 4  # Reduced from 8 heads
  
  # Vision processing (DiNOv2 features - Efficient)
  vision_encoder_dim: 768  # DiNOv2 feature dimension (fixed)
  vision_latent_size: 256  # Reduced from 512 to match episode_dim
  vision_hidden_size: 128  # Reduced from 256
  
  # Cross-modal fusion (Tiny BitNet quantized)
  fusion_hidden_size: 256  # Reduced from 512
  fusion_num_heads: 4  # Reduced from 8
  fusion_num_layers: 2  # Keep at 2 for cross-modal capability
  
  # Episodic Memory (Tiny Model - Edge Optimized)
  memory_size: 16  # Ultra-compact: 16 memory slots for edge deployment
  episode_dim: 256  # Reduced episode dimension for efficiency
  memory_alpha: 0.15  # Slightly higher for faster adaptation with fewer slots
  direct_writing: true
  
  # Model configuration (Tiny Model Optimized)
  max_seq_len: 64  # Reduced from 128 for edge efficiency
  dropout: 0.15  # Slightly higher for regularization with smaller model

data:
  # Dataset configuration (Tiny Model)
  dataset_dir: "../babylm_dataset"
  text_encoder_name: "gpt2"
  max_seq_length: 64  # Reduced from 128 for edge efficiency
  
  # DataLoader settings (Edge optimized)
  batch_size: 8  # Reduced from 32 for memory efficiency
  num_workers: 4  # Reduced from 8
  pin_memory: true
  persistent_workers: true
  
  # HuggingFace configuration (use environment variables)
  hf_token: null  # Use HF_TOKEN environment variable
  validation_datasets: 
    - "squad"  # Public dataset, no auth required
    - "glue/sst2"  # Public dataset, no auth required

# Enhanced attention analysis configuration
attention_analysis:
  # Enable attention head tracking and visualization
  track_top_k: 20  # Track top 20 most important heads
  log_every_n_steps: 100  # Analyze attention every 100 steps
  viz_every_n_epochs: 2  # Create visualizations every 2 epochs
  save_head_patterns: true  # Save attention patterns to files
  analyze_memory_attention: true  # Analyze episodic memory attention
  analyze_cross_modal: true  # Analyze cross-modal attention fusion

training:
  # Training configuration
  max_epochs: 10
  accumulate_grad_batches: 1
  gradient_clip_val: 1.0
  val_check_interval: 1000
  scheduler: "cosine"
  min_lr: 0.00001
  warmup_steps: 1000
  learning_rate: 0.0001
  weight_decay: 0.01
  track_attention: true

# Weights & Biases configuration (optional)
# Enhanced wandb configuration with comprehensive logging
wandb:
  project: "bitmar-babylm-edge"
  entity: "babylm-ntust"  # Use babylm-ntust team
  api_key: null  # Use WANDB_API_KEY environment variable
  log_every_n_steps: 50
  # Detailed logging categories with proper axis labels
  log_attention: true
  log_memory: true
  log_gradients: true
  log_quantization: true
  log_features: true
  save_code: true
  # Visualization settings
  create_plots: true
  plot_attention_heatmaps: true
  plot_memory_usage: true
  plot_quantization_dist: true

# Evaluation settings
evaluation:
  metrics: ["bleu", "rouge", "cross_modal_accuracy"]
  generate_samples: true
  num_samples: 100
  max_generation_length: 100
  temperature: 0.7
  top_p: 0.9

# Output directories (including attention analysis)
output:
  checkpoint_dir: "checkpoints"
  log_dir: "logs"
  attention_dir: "attention_analysis"
  memory_dir: "memory_analysis"
  results_dir: "results"
  attention_dir: "attention_analysis"
  memory_dir: "memory_analysis"
  results_dir: "results"

# CPU testing configuration (Tiny Model)
cpu_test:
  batch_size: 2
  max_samples: 100
  reduced_model_size: true
  memory_size: 8  # Ultra-tiny: 8 slots for CPU testing