model:
  # Text processing (BitNet-inspired)
  text_encoder_type: "bert"
  text_encoder_name: "bert-base-uncased"
  text_decoder_type: "gpt2"
  text_decoder_name: "gpt2-medium"
  text_latent_size: 768
  
  # Vision processing
  vision_encoder_dim: 768  # DiNOv2 feature dimension
  vision_latent_size: 768
  vision_quantization: true
  vision_hidden_size: 512
  
  # Episodic Memory System (Larimar-inspired)
  memory_size: 512
  episode_dim: 768
  memory_alpha: 0.1
  direct_writing: true
  ordering: false
  pseudoinverse_approx_step: 15
  observation_noise_std: 0.000001
  identity: true
  w_logvar_setting: 3
  deterministic_w: false
  
  # Cross-modal fusion
  fusion_type: "cross_attention"  # or "concat", "add"
  fusion_hidden_size: 768
  fusion_num_heads: 12
  fusion_num_layers: 2
  
  # Quantization settings
  weight_quantization: "ternary"  # {-1, 0, +1}
  activation_quantization: "int8"
  gradient_checkpointing: true
  
  # Training hyperparameters
  learning_rate: 1e-4
  weight_decay: 0.01
  warmup_steps: 1000
  gradient_clip: 1.0
  beta: 0.5
  use_beta_schedule: true
  ratio_increase: 0.25
  ratio_zero: 0.5
  
  # Attention analysis
  track_attention: true
  save_attention_patterns: true
  analyze_cross_modal: true

data:
  # Dataset paths
  captions_file: "../babylm_dataset/cc_3M_captions.json"
  vision_features_1: "../babylm_dataset/cc_3M_dino_v2_states_1of2.npy"
  vision_features_2: "../babylm_dataset/cc_3M_dino_v2_states_2of2.npy"
  
  # Data processing
  max_seq_length: 512
  vision_feature_dim: 768
  train_split: 0.95
  val_split: 0.05
  
  # DataLoader settings
  batch_size: 16
  num_workers: 4
  pin_memory: true
  persistent_workers: true

training:
  # Training configuration
  max_epochs: 10
  accumulate_grad_batches: 1
  precision: "16-mixed"
  
  # Validation and checkpointing
  val_check_interval: 0.25
  check_val_every_n_epoch: 1
  enable_checkpointing: true
  
  # Hardware settings
  accelerator: "auto"
  devices: "auto"
  strategy: "auto"
  
  # Optimization
  optimizer: "adamw"
  scheduler: "cosine"
  min_lr: 1e-6

# Wandb configuration
wandb:
  project: "bitmar-babylm"
  entity: "babylm-ntust-org"
  api_key: "5fba3726e4e32540d9fcba403f880dfaad983051"
  log_every_n_steps: 50
  log_attention: true
  log_memory: true
  log_gradients: false
  save_code: true

# Evaluation settings
evaluation:
  metrics: ["bleu", "rouge", "cross_modal_accuracy"]
  generate_samples: true
  num_samples: 100
  max_generation_length: 100
  temperature: 0.7
  top_p: 0.9

# Output directories
output:
  checkpoint_dir: "checkpoints"
  log_dir: "logs"
  attention_dir: "attention_analysis"
  memory_dir: "memory_analysis"
  results_dir: "results"

# CPU testing configuration
cpu_test:
  batch_size: 2
  max_samples: 100
  reduced_model_size: true
  memory_size: 64  # Reduced for CPU testing
