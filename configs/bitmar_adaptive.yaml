# BitMar Ultra-Tiny Edge Configuration with Adaptive Training
# Optimized for ARM Cortex-M7 and similar edge devices with <1MB RAM
# Includes aggressive DiNOv2 feature compression + Dynamic similarity monitoring

model:
  # BitNet Text Encoder (Ultra-Minimal)
  vocab_size: 50257 # GPT-2 vocabulary (cannot reduce)
  text_encoder_dim: 64 # Further reduced from 128
  text_encoder_layers: 2 # Minimal layers for edge
  text_encoder_heads: 2 # Minimal heads

  # BitNet Text Decoder (Ultra-Minimal)
  text_decoder_dim: 64 # Further reduced from 128
  text_decoder_layers: 2 # Minimal layers for edge
  text_decoder_heads: 2 # Minimal heads

  # Vision processing (Compressed DiNOv2 features)
  vision_encoder_dim: 768 # DiNOv2 input (fixed)
  vision_latent_size: 32 # Heavily compressed (768 -> 32)
  vision_hidden_size: 16 # Ultra-small hidden layer
  vision_compression_method: "top_k_selection" # Best for edge devices
  vision_spatial_pooling: true # 14x14 -> 7x7 spatial reduction
  vision_pool_size: 2 # 2x2 pooling

  # Cross-modal fusion (Ultra-Minimal)
  fusion_hidden_size: 32 # Minimal fusion
  fusion_num_heads: 1 # Single head for basic cross-modal
  fusion_num_layers: 1 # Single fusion layer

  # Episodic Memory (Ultra-Compact for Edge)
  memory_size: 8 # 8 memory slots only
  episode_dim: 32 # Match vision_latent_size
  memory_alpha: 0.2 # Reduced from 0.3 for gentler updates
  direct_writing: true
  memory_compression: true # Enable memory compression

  # Model configuration (Edge Optimized)
  max_seq_len: 256 # Increased to match data config and better utilize GPU
  dropout: 0.15 # Reduced from 0.25 for better initial convergence

  # ADAPTIVE TRAINING - Dynamic similarity monitoring
  enable_adaptive_training: true
  adaptive_similarity_monitoring: true

  # CRITICAL: Better initialization for lower initial loss
  initialization:
    # Text encoder initialization
    text_embedding_init: "normal"
    text_embedding_std: 0.01 # Very small for stability

    # Vision encoder initialization
    vision_projection_init: "xavier_uniform"
    vision_projection_gain: 0.3 # Small gain for stability

    # Cross-modal fusion initialization
    fusion_init: "kaiming_normal"
    fusion_init_mode: "fan_in"
    fusion_init_gain: 0.2 # Very conservative

    # Memory initialization
    memory_init: "zeros" # Start with zero memory for stability
    memory_query_init: "xavier_uniform"
    memory_key_init: "xavier_uniform"
    memory_value_init: "xavier_uniform"
    memory_init_gain: 0.1 # Very small for memory stability

    # BitNet quantization initialization
    bitnet_weight_init: "uniform"
    bitnet_weight_bound: 0.05 # Very small initial weights
    bitnet_scale_init: 0.01 # Small scale factor

    # Layer normalization
    layernorm_weight_init: 1.0
    layernorm_bias_init: 0.0

  # Enhanced loss configuration for multimodal training
  loss_config:
    # Use label smoothing to prevent overconfident predictions
    label_smoothing: 0.1

    # Focal loss parameters for hard example mining
    use_focal_loss: true
    focal_alpha: 0.25
    focal_gamma: 2.0

    # Cross-modal alignment loss
    cross_modal_alignment_weight: 0.1
    alignment_temperature: 0.5

    # Contrastive learning for better multimodal representation
    use_contrastive_loss: true
    contrastive_temperature: 0.1
    contrastive_margin: 0.5

# Feature reduction configuration
vision_feature_reduction:
  enabled: true
  method: "top_k_selection" # Select most important 32 dims from 768
  target_dim: 32 # Compress 768D -> 32D (24x reduction)
  spatial_pooling: true # 14x14 -> 7x7 patches (4x reduction)
  pool_method: "average" # Average pooling

  # Alternative methods (for experimentation)
  alternative_methods:
    linear_projection:
      target_dim: 32
      learnable: false
    pca_reduction:
      target_dim: 32
      explained_variance: 0.95
    learned_compression:
      target_dim: 32
      hidden_dim: 128

data:
  # Dataset configuration (High GPU Utilization)
  dataset_dir: "../babylm_dataset"  # Correct relative path from BitMar directory
  text_encoder_name: "gpt2"
  max_seq_length: 256 # Increased from 128 for better GPU utilization

  # DataLoader settings (Optimized for RTX A6000)
  batch_size: 64 # Increased from 16 for much better GPU usage (4x increase)
  num_workers: 8 # More workers for better data loading pipeline
  pin_memory: true # Enable for faster GPU transfers
  persistent_workers: true # Keep workers alive for efficiency

  # Validation (minimal)
  validation_datasets:
    - "glue/sst2" # Single lightweight dataset

# Enhanced attention analysis (minimal for edge)
attention_analysis:
  track_top_k: 2 # Track only top 2 heads
  log_every_n_steps: 500 # Infrequent logging
  viz_every_n_epochs: 10 # Rare visualization
  save_head_patterns: false # Disable to save storage
  analyze_memory_attention: false # Disable for performance
  analyze_cross_modal: true # Keep only essential analysis

training:
  # Training configuration (Optimized for lower initial loss)
  max_epochs: 50
  accumulate_grad_batches: 8
  gradient_clip_val: 1.0 # Increased from 0.5 for better stability
  val_check_interval: 5000
  scheduler: "cosine_with_restarts" # Better than plain cosine for multimodal
  min_lr: 0.000005 # Reduced minimum LR
  warmup_steps: 2000 # Increased warmup for better initialization
  learning_rate: 0.0001 # Reduced from 0.0003 - too aggressive for tiny model
  weight_decay: 0.005 # Reduced from 0.01 for stability
  track_attention: false

  # CRITICAL: Better optimizer configuration
  optimizer: "lion" # Lion optimizer often better for small models
  optimizer_config:
    betas: [0.9, 0.99] # More conservative momentum
    eps: 1e-8
    use_triton: false # Disable for compatibility

  # Loss balancing (CRITICAL for reducing initial loss)
  loss_balancing:
    text_loss_weight: 0.7 # Reduced from 1.0
    cross_modal_loss_weight: 0.2 # Reduced from default
    vision_loss_weight: 0.05 # Very small for stability
    memory_loss_weight: 0.05 # Very small for stability

    # Adaptive loss scaling
    adaptive_loss_scaling: true
    loss_temperature: 2.0 # Soften loss scaling

    # Loss clipping to prevent explosions
    max_loss_clip: 20.0 # Clip individual losses
    loss_smoothing_alpha: 0.1 # Smooth loss transitions

  # Model initialization improvements
  weight_initialization:
    method: "xavier_uniform" # Better than default for small models
    gain: 0.5 # Smaller gain for stability
    bias_init: "zero"

    # BitNet specific initialization
    bitnet_weight_scale: 0.1 # Smaller initial scale
    activation_scale: 0.5 # Smaller activation scale

  # Enhanced numerical stability
  numerical_stability:
    loss_scale: 1.0
    eps: 1e-8
    max_loss_threshold: 25.0 # Reduced from 50.0
    gradient_accumulation_steps: 8

    # Mixed precision improvements
    fp16_epsilon: 1e-4 # Larger epsilon for fp16
    loss_scaling: "dynamic" # Dynamic loss scaling

  # Scheduler improvements for multimodal
  scheduler_config:
    T_0: 1000 # First restart at 1000 steps
    T_mult: 2 # Double period each restart
    eta_min_ratio: 0.1 # Minimum LR as ratio of max_lr

# Weights & Biases (minimal logging)
wandb:
  project: "bitmar-ultra-tiny-adaptive"
  entity: "babylm-ntust"
  api_key: null
  log_every_n_steps: 500 # Very infrequent logging
  log_attention: false # Disable for performance
  log_memory: true
  log_gradients: false # Disable for performance
  log_quantization: false # Disable for performance
  log_features: false # Disable for performance
  save_code: true
  create_plots: true # Disable for performance
  plot_attention_heatmaps: true
  plot_memory_usage: false
  plot_quantization_dist: false

# Evaluation (ultra-lightweight)
evaluation:
  metrics: ["bleu"] # Single metric only
  generate_samples: false # Disable for performance
  num_samples: 5 # Minimal samples
  max_generation_length: 16 # Short generation
  temperature: 0.9
  top_p: 0.95

# Output directories
output:
  checkpoint_dir: "checkpoints_ultra_tiny"
  log_dir: "logs_ultra_tiny"
  attention_dir: "attention_ultra_tiny"
  memory_dir: "memory_ultra_tiny"
  results_dir: "results_ultra_tiny"

# Edge deployment testing
edge_test:
  batch_size: 1 # Single sample inference
  max_samples: 20 # Minimal testing
  reduced_model_size: true
  memory_size: 4 # 4 slots for testing
  enable_external_memory: false

# Memory optimization settings (aggressive)
memory_optimization:
  use_gradient_checkpointing: true # Trade compute for memory
  use_fp16: true # Half precision everywhere
  use_int8_vision: true # 8-bit vision features
  empty_cache_frequency: 5 # Aggressive cache clearing
  max_memory_slots_in_ram: 4 # Keep only 4 slots in RAM
  compress_episodic_memory: true # Compress memory slots

  # Vision feature optimization
  vision_feature_caching: false # Don't cache for memory saving
  vision_batch_processing: false # Process one image at a time

  # Model size optimization
  tie_word_embeddings: true # Share input/output embeddings
  use_shared_attention: true # Share attention weights across layers

# Performance targets for edge deployment
performance_targets:
  max_model_size_mb: 2 # 2MB total model size
  max_inference_memory_mb: 4 # 4MB inference memory
  target_inference_time_ms: 100 # 100ms inference time
  min_accuracy_threshold: 0.70 # 70% of full model accuracy

# Hardware-specific optimizations
hardware_optimization:
  target_device: "arm_cortex_m7"
  enable_neon_simd: true # ARM NEON optimization
  use_quantized_inference: true # Full quantization
  optimize_for_cache: true # Cache-friendly memory layout
  enable_model_compression: true # Additional model compression

# ADAPTIVE TRAINING CONTROLLER - Step-based dynamic similarity management
adaptive_training:
  # Core adaptive settings
  enabled: true

  # STEP-BY-STEP MONITORING (Every single step!)
  monitor_every_step: true              # Compute similarity every step
  similarity_window_size: 100           # 100-step sliding window for drop detection
  similarity_smoothing_alpha: 0.1       # Light smoothing to reduce noise

  # DROP DETECTION (15% threshold)
  drop_threshold: 0.15                  # Trigger when similarity drops >15%
  drop_detection_window: 100            # Steps to look back for drop detection
  min_steps_between_interventions: 500  # Cooldown between interventions

  # TRAINING PHASE DETECTION (Step-based thresholds)
  early_training_threshold: 50000       # Steps < 50k = early training
  mid_training_threshold: 150000        # Steps 50k-150k = mid training
  # Steps > 150k = late training

  # EARLY TRAINING INTERVENTIONS (< 50k steps)
  early_training:
    primary_strategy: "freeze_vision_encoder"    # Prevent vision dominance
    freeze_duration_steps: 2000                  # Freeze for 2000 steps
    auto_recovery: true                          # Auto-unfreeze after duration
    backup_strategies:
      - "boost_cross_modal_loss"                 # Backup: boost cross-modal loss
      - "reduce_learning_rate"                   # Last resort: reduce LR
    intervention_message: "Early training: Freezing vision encoder to prevent dominance"

  # MID TRAINING INTERVENTIONS (50k-150k steps)
  mid_training:
    primary_strategy: "freeze_text_encoder"      # Prevent text overfitting
    freeze_duration_steps: 2000                  # Freeze for 2000 steps
    auto_recovery: true                          # Auto-unfreeze after duration
    backup_strategies:
      - "freeze_vision_encoder"                  # Backup: freeze vision instead
      - "boost_cross_modal_loss"                 # Secondary: boost loss
    intervention_message: "Mid training: Freezing text encoder to prevent overfitting"

  # LATE TRAINING INTERVENTIONS (> 150k steps)
  late_training:
    primary_strategy: "gentle_loss_rebalancing"  # Only gentle adjustments
    loss_boost_factor: 1.5                      # Gentle 1.5x boost (not aggressive)
    boost_duration_steps: 2000                   # Boost for 2000 steps
    auto_recovery: true                          # Auto-reduce after duration
    backup_strategies:
      - "reduce_learning_rate"                   # Backup: slight LR reduction
    intervention_message: "Late training: Applying gentle loss rebalancing"

  # AUTO-RECOVERY SYSTEM
  auto_recovery:
    enabled: true                               # Enable automatic recovery
    unfreeze_after_steps: 2000                  # Unfreeze encoders after 2000 steps
    reduce_loss_boost_after_steps: 2000         # Reduce loss boost after 2000 steps
    recovery_check_frequency: 100               # Check recovery every 100 steps
    recovery_success_threshold: 0.70            # Consider recovered if similarity > 70%
    gradual_recovery: true                      # Gradually unfreeze/reduce boost
    recovery_steps: 200                         # Steps to gradually recover

  # SMART INTERVENTION LOGIC
  intervention_strategies:
    freeze_vision_encoder:
      description: "Freeze vision encoder parameters"
      freeze_vision_layers: true
      freeze_cross_modal_attention: false       # Keep cross-modal active
      freeze_duration: 2000

    freeze_text_encoder:
      description: "Freeze text encoder parameters"
      freeze_text_layers: true
      freeze_cross_modal_attention: false       # Keep cross-modal active
      freeze_duration: 2000

    gentle_loss_rebalancing:
      description: "Gentle cross-modal loss boost"
      cross_modal_weight_multiplier: 1.5        # Gentle boost
      boost_duration: 2000
      gradual_reduction: true

    boost_cross_modal_loss:
      description: "Strong cross-modal loss boost"
      cross_modal_weight_multiplier: 2.5        # Stronger boost for early/mid
      boost_duration: 2000
      gradual_reduction: true

    reduce_learning_rate:
      description: "Temporary learning rate reduction"
      lr_scale_factor: 0.5                      # Halve the learning rate
      duration_steps: 2000
      gradual_recovery: true

  # INTERVENTION LOGGING AND TRACKING
  log_interventions: true                       # Detailed intervention logging
  log_similarity_every_step: false             # Don't spam logs (only on drops)
  log_training_phase_changes: true             # Log when phase changes
  save_intervention_history: true              # Save history for analysis

  # INTERVENTION LIMITS (Prevent over-intervention)
  max_interventions_per_phase: 5               # Max 5 interventions per phase
  max_consecutive_interventions: 2             # Max 2 consecutive interventions
  intervention_cooldown_multiplier: 1.5        # Increase cooldown after each intervention

  # SIMILARITY RECOVERY MONITORING
  recovery_monitoring:
    enabled: true
    target_similarity: 0.75                    # Target similarity after intervention
    patience_steps: 1000                       # Wait 1000 steps for recovery
    failure_threshold: 0.50                    # Below 50% = intervention failed
    retry_with_backup: true                    # Try backup strategy if primary fails
    max_retries: 2                             # Max 2 retries per drop event

  # PHASE-SPECIFIC LOGGING
  phase_logging:
    log_phase_transitions: true
    log_strategy_selection: true
    log_recovery_progress: true
    phase_summary_frequency: 10000             # Log phase summary every 10k steps

