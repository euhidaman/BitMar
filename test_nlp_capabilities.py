#!/usr/bin/env python3
"""
BitMar NLP Capabilities Test
Tests specific natural language processing capabilities
"""

import os
import sys
import torch
import yaml
import json
import logging
import numpy as np
from pathlib import Path
import time

# Add src to Python path
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def test_nlp_capabilities():
    """Test NLP capabilities without loading the full model"""
    logger.info("üß™ Testing BitMar NLP Capabilities")
    logger.info("=" * 50)
    
    try:
        from model import BitNetLinear, BitNetAttention, create_bitmar_model
        logger.info("‚úÖ BitMar modules imported successfully")
    except ImportError as e:
        logger.error(f"‚ùå Failed to import BitMar modules: {e}")
        return False
    
    # Test 1: BitNet Linear Layer
    logger.info("\n1Ô∏è‚É£ Testing BitNet Linear Layer")
    try:
        layer = BitNetLinear(512, 256)
        test_input = torch.randn(4, 512)
        
        # Training mode
        layer.train()
        output_train = layer(test_input)
        
        # Inference mode (quantized)
        layer.eval()
        output_inference = layer(test_input)
        
        # Test quantization
        weights = layer.weight.detach()
        quantized = layer.quantize_weights_1_58_bit(weights)
        unique_vals = quantized.unique()
        
        logger.info(f"   ‚úÖ Training output shape: {output_train.shape}")
        logger.info(f"   ‚úÖ Inference output shape: {output_inference.shape}")
        logger.info(f"   ‚úÖ Quantized unique values: {unique_vals.tolist()}")
        logger.info(f"   ‚úÖ BitNet quantization working correctly")
        
    except Exception as e:
        logger.error(f"   ‚ùå BitNet Linear test failed: {e}")
        return False
    
    # Test 2: BitNet Attention
    logger.info("\n2Ô∏è‚É£ Testing BitNet Attention")
    try:
        attention = BitNetAttention(dim=512, num_heads=8)
        test_seq = torch.randn(2, 16, 512)  # batch=2, seq_len=16, dim=512
        
        output, attn_weights = attention(test_seq, test_seq, test_seq)
        
        logger.info(f"   ‚úÖ Attention output shape: {output.shape}")
        logger.info(f"   ‚úÖ Attention weights shape: {attn_weights.shape}")
        logger.info(f"   ‚úÖ BitNet attention working correctly")
        
    except Exception as e:
        logger.error(f"   ‚ùå BitNet Attention test failed: {e}")
        return False
    
    # Test 3: Model Configuration
    logger.info("\n3Ô∏è‚É£ Testing Model Configuration")
    try:
        config_path = "configs/bitmar_config.yaml"
        if os.path.exists(config_path):
            with open(config_path, 'r') as f:
                config = yaml.safe_load(f)
            
            model_config = config['model']
            required_keys = [
                'vocab_size', 'text_encoder_dim', 'text_decoder_dim',
                'vision_encoder_dim', 'fusion_hidden_size', 'memory_size'
            ]
            
            missing_keys = [key for key in required_keys if key not in model_config]
            
            if missing_keys:
                logger.warning(f"   ‚ö†Ô∏è  Missing config keys: {missing_keys}")
            else:
                logger.info(f"   ‚úÖ All required config keys present")
                logger.info(f"   ‚úÖ Vocab size: {model_config['vocab_size']:,}")
                logger.info(f"   ‚úÖ Text encoder dim: {model_config['text_encoder_dim']}")
                logger.info(f"   ‚úÖ Memory slots: {model_config['memory_size']}")
        else:
            logger.error(f"   ‚ùå Config file not found: {config_path}")
            return False
            
    except Exception as e:
        logger.error(f"   ‚ùå Config test failed: {e}")
        return False
    
    # Test 4: Text Generation Capability
    logger.info("\n4Ô∏è‚É£ Testing Text Generation Setup")
    try:
        # Test tokenizer setup
        from transformers import AutoTokenizer
        tokenizer = AutoTokenizer.from_pretrained('gpt2')
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # Test encoding
        test_text = "The future of artificial intelligence"
        encoded = tokenizer(test_text, return_tensors="pt", max_length=32)
        
        logger.info(f"   ‚úÖ Tokenizer loaded successfully")
        logger.info(f"   ‚úÖ Test text encoded: {encoded['input_ids'].shape}")
        logger.info(f"   ‚úÖ Vocab size: {tokenizer.vocab_size:,}")
        
    except Exception as e:
        logger.error(f"   ‚ùå Text generation setup failed: {e}")
        return False
    
    # Test 5: Memory Efficiency
    logger.info("\n5Ô∏è‚É£ Testing Memory Efficiency")
    try:
        import psutil
        process = psutil.Process()
        
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # Create and delete some layers
        layers = []
        for _ in range(10):
            layer = BitNetLinear(512, 512)
            layers.append(layer)
        
        peak_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # Clean up
        del layers
        import gc
        gc.collect()
        
        final_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        logger.info(f"   ‚úÖ Initial memory: {initial_memory:.1f} MB")
        logger.info(f"   ‚úÖ Peak memory: {peak_memory:.1f} MB")
        logger.info(f"   ‚úÖ Final memory: {final_memory:.1f} MB")
        logger.info(f"   ‚úÖ Memory usage reasonable")
        
    except Exception as e:
        logger.error(f"   ‚ùå Memory efficiency test failed: {e}")
        return False
    
    # Test 6: GPU Compatibility
    logger.info("\n6Ô∏è‚É£ Testing GPU Compatibility")
    try:
        gpu_available = torch.cuda.is_available()
        device = torch.device("cuda" if gpu_available else "cpu")
        
        if gpu_available:
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
            logger.info(f"   ‚úÖ GPU available: {gpu_name}")
            logger.info(f"   ‚úÖ GPU memory: {gpu_memory:.1f} GB")
            
            # Test moving tensor to GPU
            test_tensor = torch.randn(100, 100).to(device)
            logger.info(f"   ‚úÖ Tensor moved to GPU successfully")
        else:
            logger.info(f"   ‚ÑπÔ∏è  No GPU available, using CPU")
            logger.info(f"   ‚úÖ CPU device ready")
        
        logger.info(f"   ‚úÖ Device: {device}")
        
    except Exception as e:
        logger.error(f"   ‚ùå GPU compatibility test failed: {e}")
        return False
    
    logger.info("\n" + "=" * 50)
    logger.info("üéâ All NLP Capability Tests Passed!")
    logger.info("=" * 50)
    
    return True


def test_text_processing_pipeline():
    """Test a simple text processing pipeline"""
    logger.info("\nüîÑ Testing Text Processing Pipeline")
    
    try:
        from transformers import AutoTokenizer
        
        # Initialize tokenizer
        tokenizer = AutoTokenizer.from_pretrained('gpt2')
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # Test texts
        test_texts = [
            "The cat sat on the mat.",
            "Artificial intelligence is transforming the world.",
            "Climate change requires immediate action.",
        ]
        
        # Process texts
        processed_results = []
        
        for text in test_texts:
            # Tokenize
            tokens = tokenizer(text, return_tensors="pt", max_length=32, padding=True)
            
            # Basic statistics
            num_tokens = tokens['input_ids'].size(1)
            decoded = tokenizer.decode(tokens['input_ids'][0], skip_special_tokens=True)
            
            result = {
                'original': text,
                'num_tokens': num_tokens,
                'decoded': decoded,
                'matches_original': text.strip().lower() == decoded.strip().lower()
            }
            
            processed_results.append(result)
        
        # Report results
        all_match = all(r['matches_original'] for r in processed_results)
        avg_tokens = np.mean([r['num_tokens'] for r in processed_results])
        
        logger.info(f"   ‚úÖ Processed {len(test_texts)} texts")
        logger.info(f"   ‚úÖ Average tokens: {avg_tokens:.1f}")
        logger.info(f"   ‚úÖ All texts decoded correctly: {all_match}")
        
        # Show sample
        sample = processed_results[0]
        logger.info(f"   üìù Sample: '{sample['original']}'")
        logger.info(f"   üî¢ Tokens: {sample['num_tokens']}")
        
        return True
        
    except Exception as e:
        logger.error(f"   ‚ùå Text processing pipeline failed: {e}")
        return False


def main():
    """Main test function"""
    logger.info("üöÄ BitMar NLP Testing Suite")
    logger.info("Testing core NLP capabilities before full model evaluation")
    
    start_time = time.time()
    
    # Run capability tests
    capabilities_passed = test_nlp_capabilities()
    
    # Run pipeline test
    pipeline_passed = test_text_processing_pipeline()
    
    total_time = time.time() - start_time
    
    # Summary
    logger.info("\n" + "=" * 50)
    logger.info("üìä Test Summary")
    logger.info("=" * 50)
    logger.info(f"‚è±Ô∏è  Total test time: {total_time:.2f}s")
    logger.info(f"‚úÖ Core capabilities: {'PASSED' if capabilities_passed else 'FAILED'}")
    logger.info(f"‚úÖ Text pipeline: {'PASSED' if pipeline_passed else 'FAILED'}")
    
    if capabilities_passed and pipeline_passed:
        logger.info("\nüéâ BitMar is ready for NLP benchmarking!")
        logger.info("Next steps:")
        logger.info("1. Run: python test_quick_benchmarks.py")
        logger.info("2. Run: python test_text_benchmarks.py")
        logger.info("3. Start training: python train_bitmar.py")
        return 0
    else:
        logger.error("\n‚ùå Some tests failed. Check the logs above.")
        return 1


if __name__ == "__main__":
    exit(main())
